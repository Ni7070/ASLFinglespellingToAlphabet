# -*- coding: utf-8 -*-
"""Fingerspelling Interpretation from ASL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p3yiwqb0aaD7G29iZlJduIKym7Rp2sR8
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
from torchvision.transforms import transforms
from torchvision import datasets
from torch.utils.data import DataLoader
from torch.utils.data import Dataset


import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
from PIL import Image

import os
from os import path
import shutil
import glob
import cv2

!pip install kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d grassknoted/asl-alphabet

from zipfile import ZipFile
file_name = "asl-alphabet.zip"

with ZipFile(file_name, 'r') as zip:
  zip.extractall()
  print("Done!")

#colab mount
from google.colab import drive
#drive.flush_and_unmount()
drive.mount('/content/gdrive')

train_dir = '/content/asl_alphabet_train/asl_alphabet_train'
test_dir = '/content/asl_alphabet_test/asl_alphabet_test'
test_d_dir = '/content/gdrive/MyDrive/Colab Notebooks/asl_alphabet_test/asl_alphabet_test'

from torchvision import datasets, transforms
import torch
mean = [0.5,]
std = [0.5, ]
transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=1),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean, std)
    
])
      
    
dataset = datasets.ImageFolder(root=train_dir, transform=transform)

len(dataset)

test_size = 0.2
torch.manual_seed(1)
num_train_samples = len(dataset)
indices = torch.randperm(num_train_samples)

split = int(num_train_samples * test_size)

train_dataset = torch.utils.data.Subset(dataset, indices[split:])
test_dataset = torch.utils.data.Subset(dataset, indices[:split])

len(train_dataset), len(test_dataset)

# Hyperparameters

batch_size = 100
num_iters = 3000
input_dim = 224*224 # num_features = 784
num_hidden = 100 # num of hidden nodes
output_dim = 29
learning_rate = 0.001

num_epochs = num_iters / (len(train_dataset) / batch_size)
num_epochs = int(num_epochs)

train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                               batch_size=batch_size, 
                                               shuffle=True, 
                                               )

test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                              batch_size=batch_size,
                                              shuffle=False,
                                              )

#label of the train classes
train_classes = train_loader.dataset.dataset.classes
test_classes = test_loader.dataset.dataset.classes
print("train classes labels: ",train_classes)
print("test classes labels: ",test_classes)

#number of data in each classes
class_count = {}
for _, index in dataset:
    label = train_classes[index]
    if label not in class_count:
        class_count[label] = 0
    class_count[label] += 1
class_count

#displaying 1 image from each train class
plt.figure(figsize=(15, 15))
for i in range (0,29):
    plt.subplot(7,7,i+1)
    plt.xticks([])
    plt.yticks([])
    path = train_dir + "/{0}/{0}1.jpg".format(train_classes[i])
    img = plt.imread(path)
    plt.imshow(img)
    plt.xlabel("Ground Truth: "+train_classes[i])

#classes = train_loader.dataset.dataset.classes
for img, label in train_loader:
    print('Ground truth', train_classes[label[0]])
    plt.imshow(img[0].reshape(224, 224), cmap = "gray")
    plt.show()
    break

"""**Demo 01:**"""

# Hyperparameters
batch_size = 128
num_iters = 3000
input_dim = 224*224 # num_features = 784
num_hidden = 120
output_dim = 29

learning_rate = 0.001

# Device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


num_epochs = num_iters / (len(train_dataset) / batch_size)
num_epochs = int(num_epochs)

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                           batch_size=batch_size, 
                                           shuffle=True)   # It's better to shuffle the whole training dataset! 

test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False) 

class NeuralNetworkModel(nn.Module):
    def __init__(self, input_size, num_classes, num_hidden):
        super().__init__()
      
        self.linear_1 = nn.Linear(input_dim, num_hidden)
        ### Non-linearity in 1st hidden layer
        self.tanh = nn.Tanh()

        ### 2nd hidden layer:
        self.linear_2 = nn.Linear(num_hidden, num_hidden)
        ### Non-linearity in 2nd hidden layer
        self.leaky_relu_1 = nn.LeakyReLU()

        ### 3rd hidden layer: 
        self.linear_3 = nn.Linear(num_hidden, num_hidden)
        ### Non-linearity in 3rd hidden layer
        self.relu_1 = nn.ReLU6()


        self.linear_5 = nn.Linear(num_hidden, num_hidden)
        self.sigmoid = nn.Sigmoid()

        ### Output layer: 
        self.linear_out = nn.Linear(num_hidden, num_classes)

    def forward(self, x):
        ## 1st hidden layer
        out  = self.linear_1(x)
        ### Non-linearity in 1st hidden layer
        out = self.tanh(out)
        
        ### 2nd hidden layer
        out  = self.linear_2(out)
        ### Non-linearity in 2nd hidden layer
        out = self.leaky_relu_1(out)

        ### 3rd hidden layer
        out  = self.linear_3(out)
        ### Non-linearity in 3rd hidden layer
        out = self.relu_1(out)

        ### 4th hidden layer 
        out  = self.linear_5(out)
        ### Non-linearity in 4th hidden layer
        out = self.sigmoid(out)
        
        # Linear layer (output)
        logits  = self.linear_out(out)
        probas = F.softmax(logits, dim=1)
        return probas


model = NeuralNetworkModel(input_size = input_dim,
                           num_classes = output_dim,
                           num_hidden = num_hidden)
# To enable GPU
model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)
'''
TRAIN THE MODEL
'''
iter = 0
iteration_loss = []
for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):

        images = images.view(-1, 224*224).to(device)
        labels = labels.to(device)

        # Clear gradients w.r.t. parameters
        optimizer.zero_grad()

        # Forward pass to get output/logits
        outputs = model(images.float()) 

        # Calculate Loss: softmax --> cross entropy loss
        loss = criterion(outputs, labels)

        # Getting gradients w.r.t. parameters
        loss.backward()

        # Updating parameters
        optimizer.step()

        iter += 1

        if iter % 500 == 0:
            # Calculate Accuracy         
            correct = 0
            total = 0
            # Iterate through test dataset
            for images, labels in test_loader:
               
                images = images.view(-1, 224*224).to(device)

                # Forward pass only to get logits/output
                outputs = model(images.float())

                # Get predictions from the maximum value
                _, predicted = torch.max(outputs, 1)

                # Total number of labels
                total += labels.size(0)


                # Total correct predictions
                if torch.cuda.is_available():
                    correct += (predicted.cpu() == labels.cpu()).sum() 
                else:
                    correct += (predicted == labels).sum()

            accuracy = 100 * correct.item() / total

            # Print Loss
            iteration_loss.append(loss.item())
            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))

import matplotlib
import matplotlib.pyplot as plt

print (iteration_loss)
plt.plot(iteration_loss)
plt.ylabel('Cross Entropy Loss')
plt.xlabel('Iteration (in every 500)')
plt.show()

"""**demo :02**"""

# Hyperparameters
batch_size = 128
num_iters = 3000
input_dim = 224*224 # num_features = 784
num_hidden = 120
output_dim = 29

learning_rate = 0.01

# Device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


num_epochs = num_iters / (len(train_dataset) / batch_size)
num_epochs = int(num_epochs)

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                           batch_size=batch_size, 
                                           shuffle=True)   # It's better to shuffle the whole training dataset! 

test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False) 

class NeuralNetworkModel(nn.Module):
    def __init__(self, input_size, num_classes, num_hidden):
        super().__init__()
      
        self.linear_1 = nn.Linear(input_dim, num_hidden)
        ### Non-linearity in 1st hidden layer
        self.tanh = nn.Tanh()

        ### 2nd hidden layer:
        self.linear_2 = nn.Linear(num_hidden, num_hidden)
        ### Non-linearity in 2nd hidden layer
        self.leaky_relu_1 = nn.LeakyReLU()

        ### 3rd hidden layer: 
        self.linear_3 = nn.Linear(num_hidden, num_hidden)
        ### Non-linearity in 3rd hidden layer
        self.relu_1 = nn.ReLU6()


        self.linear_5 = nn.Linear(num_hidden, num_hidden)
        self.sigmoid = nn.Sigmoid()

        ### Output layer: 
        self.linear_out = nn.Linear(num_hidden, num_classes)

    def forward(self, x):
        ## 1st hidden layer
        out  = self.linear_1(x)
        ### Non-linearity in 1st hidden layer
        out = self.tanh(out)
        
        ### 2nd hidden layer
        out  = self.linear_2(out)
        ### Non-linearity in 2nd hidden layer
        out = self.leaky_relu_1(out)

        ### 3rd hidden layer
        out  = self.linear_3(out)
        ### Non-linearity in 3rd hidden layer
        out = self.relu_1(out)

        ### 4th hidden layer 
        out  = self.linear_5(out)
        ### Non-linearity in 4th hidden layer
        out = self.sigmoid(out)
        
        # Linear layer (output)
        logits  = self.linear_out(out)
        probas = F.softmax(logits, dim=1)
        return probas


model = NeuralNetworkModel(input_size = input_dim,
                           num_classes = output_dim,
                           num_hidden = num_hidden)
# To enable GPU
model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)
'''
TRAIN THE MODEL
'''
iter = 0
iteration_loss = []
for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):

        images = images.view(-1, 224*224).to(device)
        labels = labels.to(device)

        # Clear gradients w.r.t. parameters
        optimizer.zero_grad()

        # Forward pass to get output/logits
        outputs = model(images.float()) 

        # Calculate Loss: softmax --> cross entropy loss
        loss = criterion(outputs, labels)

        # Getting gradients w.r.t. parameters
        loss.backward()

        # Updating parameters
        optimizer.step()

        iter += 1

        if iter % 500 == 0:
            # Calculate Accuracy         
            correct = 0
            total = 0
            # Iterate through test dataset
            for images, labels in test_loader:
               
                images = images.view(-1, 224*224).to(device)

                # Forward pass only to get logits/output
                outputs = model(images.float())

                # Get predictions from the maximum value
                _, predicted = torch.max(outputs, 1)

                # Total number of labels
                total += labels.size(0)


                # Total correct predictions
                if torch.cuda.is_available():
                    correct += (predicted.cpu() == labels.cpu()).sum() 
                else:
                    correct += (predicted == labels).sum()

            accuracy = 100 * correct.item() / total

            # Print Loss
            iteration_loss.append(loss.item())
            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))

import matplotlib
import matplotlib.pyplot as plt

print (iteration_loss)
plt.plot(iteration_loss)
plt.ylabel('Cross Entropy Loss')
plt.xlabel('Iteration (in every 500)')
plt.show()

"""**Demo 03**"""

# Hyperparameters 1st
batch_size = 120
num_iters = 3000
input_dim = 224*224 # num_features = 784
output_dim = 29

learning_rate = 0.01


num_epochs = num_iters / (len(train_dataset) / batch_size)
num_epochs = int(num_epochs)


# Device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")



train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                           batch_size=batch_size, 
                                           shuffle=True
                                           )  

test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False) 

class NeuralNetworkModel(nn.Module):
    def __init__(self, input_size, num_classes, num_hidden):
        super().__init__()
        self.fc1 = nn.Linear(input_size, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 128)
        self.fc4 = nn.Linear(128, num_classes)

    def forward(self, x):
        x = x.view(x.shape[0], -1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = F.log_softmax(self.fc4(x), dim=1)
        return x

model = NeuralNetworkModel(input_size = input_dim,
                           num_classes = output_dim,
                           num_hidden = num_hidden)
# To enable GPU
model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)

'''
TRAIN THE MODEL
'''
iteration_loss = []
iter = 0
p = []
l = []
for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):

        images = images.view(-1, 224*224).to(device)
        labels = labels.to(device)

        # Clear gradients w.r.t. parameters
        optimizer.zero_grad()

        # Forward pass to get output/logits
        outputs = model(images.float()) 

        # Calculate Loss: softmax --> cross entropy loss
        loss = criterion(outputs, labels)

        # Getting gradients w.r.t. parameters
        loss.backward()

        # Updating parameters
        optimizer.step()

        iter += 1
        #print(iter)

        if iter % 500 == 0:
            # Calculate Accuracy         
            correct = 0
            total = 0
            # Iterate through test dataset
            for images, labels in test_loader:
               
                images = images.view(-1, 224*224).to(device)

                # Forward pass only to get logits/output
                outputs = model(images.float())

                # Get predictions from the maximum value
                _, predicted = torch.max(outputs, 1)

                # Total number of labels
                total += labels.size(0)

                pt = predicted.cpu()
                lt = labels.cpu()
                #taking all predicted label into a list
                p.append(pt)
                #taking all real label into a list
                l.append(lt)


                # Total correct predictions
                if torch.cuda.is_available():
                    correct += (predicted.cpu() == labels.cpu()).sum() 
                else:
                    correct += (predicted == labels).sum()

            accuracy = 100 * correct.item() / total

            # Print Loss
            iteration_loss.append(loss.item())
            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))

print (iteration_loss)
plt.plot(iteration_loss)
plt.ylabel('Cross Entropy Loss')
plt.xlabel('Iteration (in every 500)')
plt.show()

"""**Settings: 01**"""

# Hyperparameters 1st
batch_size = 120
num_iters = 3000
input_dim = 224*224 # num_features = 784
num_hidden = 120
output_dim = 29

learning_rate = 0.01


num_epochs = num_iters / (len(train_dataset) / batch_size)
num_epochs = int(num_epochs)


# Device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")



train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                           batch_size=batch_size, 
                                           shuffle=True
                                           )  

test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False) 

class NeuralNetworkModel(nn.Module):
    def __init__(self, input_size, num_classes, num_hidden):
        super().__init__()
        self.fc1 = nn.Linear(input_size, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 128)
        self.fc4 = nn.Linear(128, num_classes)

    def forward(self, x):
        x = x.view(x.shape[0], -1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = F.log_softmax(self.fc4(x), dim=1)
        return x

model = NeuralNetworkModel(input_size = input_dim,
                           num_classes = output_dim,
                           num_hidden = num_hidden)
# To enable GPU
model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)

'''
TRAIN THE MODEL
'''
iteration_loss = []
iter = 0
p = []
l = []
for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):

        images = images.view(-1, 224*224).to(device)
        labels = labels.to(device)

        # Clear gradients w.r.t. parameters
        optimizer.zero_grad()

        # Forward pass to get output/logits
        outputs = model(images.float()) 

        # Calculate Loss: softmax --> cross entropy loss
        loss = criterion(outputs, labels)

        # Getting gradients w.r.t. parameters
        loss.backward()

        # Updating parameters
        optimizer.step()

        iter += 1
        #print(iter)

        if iter % 500 == 0:
            # Calculate Accuracy         
            correct = 0
            total = 0
            # Iterate through test dataset
            for images, labels in test_loader:
               
                images = images.view(-1, 224*224).to(device)

                # Forward pass only to get logits/output
                outputs = model(images.float())

                # Get predictions from the maximum value
                _, predicted = torch.max(outputs, 1)

                # Total number of labels
                total += labels.size(0)

                pt = predicted.cpu()
                lt = labels.cpu()
                #taking all predicted label into a list
                p.append(pt)
                #taking all real label into a list
                l.append(lt)


                # Total correct predictions
                if torch.cuda.is_available():
                    correct += (predicted.cpu() == labels.cpu()).sum() 
                else:
                    correct += (predicted == labels).sum()

            accuracy = 100 * correct.item() / total

            # Print Loss
            iteration_loss.append(loss.item())
            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))

import matplotlib
import matplotlib.pyplot as plt

print (iteration_loss)
plt.plot(iteration_loss)
plt.ylabel('Cross Entropy Loss')
plt.xlabel('Iteration (in every 500)')
plt.show()

import itertools
import seaborn as sns


#flatten predicted labels and real labels
flatten_l = list(itertools.chain.from_iterable(l))
flatten_p = list(itertools.chain.from_iterable(p))

fig, ax = plt.subplots(figsize=(16, 16))
from sklearn.metrics import confusion_matrix
# Confusion matrix
conf_mat=confusion_matrix(flatten_l, flatten_p)
# print(conf_mat)
sns.heatmap(conf_mat, annot=True)

print(classification_report(flatten_l, flatten_p))

root_path = "/content/gdrive/MyDrive/Colab Notebooks/pickle Files/"
save_model = True
if save_model is True:
    # Saves only parameters
    # wights & biases
    torch.save(model.state_dict(), root_path + 'asl02.pkl')

load_model = True

if load_model is True:
    model.load_state_dict(torch.load(root_path + 'asl02.pkl'))

#For test data
testing_dataset = datasets.ImageFolder(root=test_d_dir, transform=transform)
testing_loader = torch.utils.data.DataLoader(dataset=testing_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False) 
for images, labels in testing_loader:
  break

predicted =  model.forward(images[:29].view(-1, 224*224).to(device))
predicted = torch.argmax(predicted, dim=1)

# print('Predicted: \n', ' '.join('%29s\n' % classes[predicted[j]]
#                                for j in range(29)))

fig, ax = plt.subplots(figsize=(15, 15))
for i in range (0,29):
    plt.subplot(7,7,i+1)
    plt.xticks([])
    plt.yticks([])
    path = test_d_dir + "/{0}/{0}_test.jpg".format(test_classes[i])
    img = plt.imread(path)
    plt.imshow(img)
    plt.xlabel("real:"+test_classes[i]+" pred:"+test_classes[predicted[i]])

rcount = 0
wcount = 0
for x in range (0,29):
  if test_classes[x] == test_classes[predicted[x]]:
    rcount = rcount + 1
  else:
    wcount = wcount + 1
rp = (rcount/29)*100
wp = 100-rp
print("Right:",rcount," Wrong: " , wcount, "\nright Assumed:", rp,"%", "Wrong Assumed:", wp,"%")

"""**Settings: 02**"""

# Hyperparameters 1st
batch_size = 120
num_iters = 4000
input_dim = 224*224 # num_features = 784

output_dim = 29

learning_rate = 0.01


num_epochs = num_iters / (len(train_dataset) / batch_size)
num_epochs = int(num_epochs)


# Device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")



train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                           batch_size=batch_size, 
                                           shuffle=True
                                           )  

test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False) 

class NeuralNetworkModel(nn.Module):
    def __init__(self, input_size, num_classes, num_hidden):
        super().__init__()
        self.fc1 = nn.Linear(input_size, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 128)
        self.fc4 = nn.Linear(128, num_classes)

    def forward(self, x):
        x = x.view(x.shape[0], -1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = F.log_softmax(self.fc4(x), dim=1)
        return x

model = NeuralNetworkModel(input_size = input_dim,
                           num_classes = output_dim,
                           num_hidden = num_hidden)
# To enable GPU
model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)

'''
TRAIN THE MODEL
'''
iteration_loss = []
iter = 0
p = []
l = []
for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):

        images = images.view(-1, 224*224).to(device)
        labels = labels.to(device)

        # Clear gradients w.r.t. parameters
        optimizer.zero_grad()

        # Forward pass to get output/logits
        outputs = model(images.float()) 

        # Calculate Loss: softmax --> cross entropy loss
        loss = criterion(outputs, labels)

        # Getting gradients w.r.t. parameters
        loss.backward()

        # Updating parameters
        optimizer.step()

        iter += 1
        #print(iter)

        if iter % 500 == 0:
            # Calculate Accuracy         
            correct = 0
            total = 0
            # Iterate through test dataset
            for images, labels in test_loader:
               
                images = images.view(-1, 224*224).to(device)

                # Forward pass only to get logits/output
                outputs = model(images.float())

                # Get predictions from the maximum value
                _, predicted = torch.max(outputs, 1)

                # Total number of labels
                total += labels.size(0)

                pt = predicted.cpu()
                lt = labels.cpu()
                #taking all predicted label into a list
                p.append(pt)
                #taking all real label into a list
                l.append(lt)


                # Total correct predictions
                if torch.cuda.is_available():
                    correct += (predicted.cpu() == labels.cpu()).sum() 
                else:
                    correct += (predicted == labels).sum()

            accuracy = 100 * correct.item() / total

            # Print Loss
            iteration_loss.append(loss.item())
            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))

import matplotlib
import matplotlib.pyplot as plt

print (iteration_loss)
plt.plot(iteration_loss)
plt.ylabel('Cross Entropy Loss')
plt.xlabel('Iteration (in every 500)')
plt.show()

import itertools
import seaborn as sns


#flatten predicted labels and real labels
flatten_l = list(itertools.chain.from_iterable(l))
flatten_p = list(itertools.chain.from_iterable(p))

fig, ax = plt.subplots(figsize=(16, 16))
from sklearn.metrics import confusion_matrix
# Confusion matrix
conf_mat=confusion_matrix(flatten_l, flatten_p)
# print(conf_mat)
sns.heatmap(conf_mat, annot=True)

print(classification_report(flatten_l, flatten_p))

root_path = "/content/gdrive/MyDrive/Colab Notebooks/pickle Files"
save_model = True
if save_model is True:
    # Saves only parameters
    # wights & biases
    torch.save(model.state_dict(), root_path + 'asl03.pkl')

load_model = True

if load_model is True:
    model.load_state_dict(torch.load(root_path + 'asl03.pkl'))

#For test data
testing_dataset = datasets.ImageFolder(root=test_d_dir, transform=transform)
testing_loader = torch.utils.data.DataLoader(dataset=testing_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False) 
for images, labels in testing_loader:
  break

predicted =  model.forward(images[:29].view(-1, 224*224).to(device))
predicted = torch.argmax(predicted, dim=1)



fig, ax = plt.subplots(figsize=(15, 15))
for i in range (0,29):
    plt.subplot(7,7,i+1)
    plt.xticks([])
    plt.yticks([])
    path = test_d_dir + "/{0}/{0}_test.jpg".format(test_classes[i])
    img = plt.imread(path)
    plt.imshow(img)
    plt.xlabel("real:"+test_classes[i]+" pred:"+test_classes[predicted[i]])

rcount = 0
wcount = 0
for x in range (0,29):
  if test_classes[x] == test_classes[predicted[x]]:
    rcount = rcount + 1
  else:
    wcount = wcount + 1
rp = (rcount/29)*100
wp = 100-rp
print("Right:",rcount," Wrong: " , wcount, "\nright Assumed:", rp,"%", "Wrong Assumed:", wp,"%")

"""**Setting: 03**"""

# Hyperparameters 1st
batch_size = 160
num_iters = 4000
input_dim = 224*224 # num_features = 784

output_dim = 29

learning_rate = 0.01


num_epochs = num_iters / (len(train_dataset) / batch_size)
num_epochs = int(num_epochs)


# Device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")



train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                           batch_size=batch_size, 
                                           shuffle=True
                                           )  

test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False) 

class NeuralNetworkModel(nn.Module):
    def __init__(self, input_size, num_classes, num_hidden):
        super().__init__()
        self.fc1 = nn.Linear(input_size, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 128)
        self.fc4 = nn.Linear(128, num_classes)

    def forward(self, x):
        x = x.view(x.shape[0], -1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = F.log_softmax(self.fc4(x), dim=1)
        return x

model = NeuralNetworkModel(input_size = input_dim,
                           num_classes = output_dim,
                           num_hidden = num_hidden)
# To enable GPU
model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)

'''
TRAIN THE MODEL
'''
iteration_loss = []
iter = 0
p = []
l = []
for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):

        images = images.view(-1, 224*224).to(device)
        labels = labels.to(device)

        # Clear gradients w.r.t. parameters
        optimizer.zero_grad()

        # Forward pass to get output/logits
        outputs = model(images.float()) 

        # Calculate Loss: softmax --> cross entropy loss
        loss = criterion(outputs, labels)

        # Getting gradients w.r.t. parameters
        loss.backward()

        # Updating parameters
        optimizer.step()

        iter += 1
        #print(iter)

        if iter % 500 == 0:
            # Calculate Accuracy         
            correct = 0
            total = 0
            # Iterate through test dataset
            for images, labels in test_loader:
               
                images = images.view(-1, 224*224).to(device)

                # Forward pass only to get logits/output
                outputs = model(images.float())

                # Get predictions from the maximum value
                _, predicted = torch.max(outputs, 1)

                # Total number of labels
                total += labels.size(0)

                pt = predicted.cpu()
                lt = labels.cpu()
                #taking all predicted label into a list
                p.append(pt)
                #taking all real label into a list
                l.append(lt)


                # Total correct predictions
                if torch.cuda.is_available():
                    correct += (predicted.cpu() == labels.cpu()).sum() 
                else:
                    correct += (predicted == labels).sum()

            accuracy = 100 * correct.item() / total

            # Print Loss
            iteration_loss.append(loss.item())
            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))

import matplotlib
import matplotlib.pyplot as plt

print (iteration_loss)
plt.plot(iteration_loss)
plt.ylabel('Cross Entropy Loss')
plt.xlabel('Iteration (in every 500)')
plt.show()

import itertools
import seaborn as sns


#flatten predicted labels and real labels
flatten_l = list(itertools.chain.from_iterable(l))
flatten_p = list(itertools.chain.from_iterable(p))

fig, ax = plt.subplots(figsize=(16, 16))
from sklearn.metrics import confusion_matrix
# Confusion matrix
conf_mat=confusion_matrix(flatten_l, flatten_p)
# print(conf_mat)
sns.heatmap(conf_mat, annot=True)

print(classification_report(flatten_l, flatten_p))

root_path = "/content/gdrive/MyDrive/Colab Notebooks/pickle Files"
save_model = True
if save_model is True:
    # Saves only parameters
    # wights & biases
    torch.save(model.state_dict(), root_path + 'asl04.pkl')

load_model = True

if load_model is True:
    model.load_state_dict(torch.load(root_path + 'asl04.pkl'))
    print('Trained Model Loaded')

#For test data
testing_dataset = datasets.ImageFolder(root=test_d_dir, transform=transform)
testing_loader = torch.utils.data.DataLoader(dataset=testing_dataset, 
                                          batch_size=batch_size, 
                                          shuffle=False) 
for images, labels in testing_loader:
  break

predicted =  model.forward(images[:29].view(-1, 224*224).to(device))
predicted = torch.argmax(predicted, dim=1)



fig, ax = plt.subplots(figsize=(15, 15))
for i in range (0,29):
    plt.subplot(7,7,i+1)
    plt.xticks([])
    plt.yticks([])
    path = test_d_dir + "/{0}/{0}_test.jpg".format(test_classes[i])
    img = plt.imread(path)
    plt.imshow(img)
    plt.xlabel("real:"+test_classes[i]+" pred:"+test_classes[predicted[i]])

rcount = 0
wcount = 0
for x in range (0,29):
  if test_classes[x] == test_classes[predicted[x]]:
    rcount = rcount + 1
  else:
    wcount = wcount + 1
rp = (rcount/29)*100
wp = 100-rp
print("Right:",rcount," Wrong: " , wcount, "\nright Assumed:", rp,"%", "Wrong Assumed:", wp,"%")

"""No.|batch_size | num_iters | learning rate|optimizer|Accuracy
---|---|---|---|---|---|
1|120|3000|0.01|Adagrad|97.29885057471265%
2|120|4000|0.01|Adagrad|98.79885057471265%
3|160|4000|0.01|Adagrad|98.95402298850574%

"""